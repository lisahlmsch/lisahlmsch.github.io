---
title: "KAGGLE COMPETITION article shares on social media"
author: "Lisa Lang (1902224)"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    highlight: vignette
    theme: leonids
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)

library(skimr) # skim
library(ggplot2)
library(data.table)
library(caret)
library(dplyr)
library(tidyr)
library(tidyverse)
library(pROC)
library(ROCR)
library(h2o)
library(GGally)
library(knitr)
library(kableExtra)
h2o.init(max_mem_size = '8g')
h2o.no_progress()
```

## Description of the Kaggle competition

This is a private binary classification competition for the "Machine Learning" course, part of CEU BA, spring semester of 2020.

In this competition, your task is to predict which articles are shared the most in social media. The data comes from website mashable.com from the beginning of 2015. The dataset used in the competition can be found at the UCI repository (https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity#) - of course, you should not cheat by checking out the whole dataset found there.

Make sure you submit at least the following 4 types of solutions:

* linear model prediction after parameter tuning
* random forest prediction after parameter tuning
* gradient boosting prediction after parameter tuning
* neural network prediction after parameter tuning.

Your best model will need to have at least 0.65 AUC to consider this part of the exam complete.

For extra points, build a stacked model, explain how it works and evaluate its results.

## Setup

```{r data_import}
test_data <- read.csv("ceuba2020/test.csv/test.csv")
train_data <- read.csv("ceuba2020/train.csv/train.csv")
```

The *training data* table consists of 27752 observations and 60 variables. The *business question* is to predict the number of shares in social networks (popularity), so the *y-variable* in this case is `is_popular`. The *target observations* are the test data in the private leaderboard of the kaggle competition.

## Data exploration and cleaning

First I am going to get an overview of the attributes in this dataset.

```{r glimpse, size = 'tiny'}
skim(train_data)
```

```{r eval=FALSE, include=FALSE}
# Attribut information:
# https://archive.ics.uci.edu/ml/datasets/Online+News+Popularity#
# 
# Number of Attributes: 61 (58 predictive attributes, 2 non-predictive, 1 goal field)
```

*Attribute description:*  
The features fall into various categories such as quantitative information about the article — such things as number of images, number of videos, etc. — and qualitative information about the article — such as which day it was published and which topic the article falls under.

*Missing values:*  
There are no missing values in any of the observations.

*Factorization:*  
For binary prediction with caret, the target variable must be a factor so I am going to coerce it to factors.
I previously tried to factorize also the independent variables, but ran into problems compiling the h2o stacked ensemble model, so I will keep them as integer values.

```{r factor_vars}
# turn character variables to factors
# colnames <- names(train_data)[grep("_is_", names(train_data))]
# 
# train_data[colnames] <- lapply(train_data[colnames], as.factor)
# test_data[colnames] <- lapply(test_data[colnames], as.factor)

train_data <- data.table(train_data)
train_data[, is_popular_f := factor(is_popular, levels = c(0,1), 
                                    labels = c("no", "yes"))]
```

Now I look at the outcome variable `is_popular`: 

```{r plot_y}
ggplot(data = train_data, aes(x = is_popular)) + geom_bar()
```

The majority of articles was labeled unpopular.

In addition I created a data profiling report with the 'DataExplorer' package, in order to have a better understanding of the dataset.

```{r data_explorer, eval = FALSE}
library(DataExplorer)
create_report(train_data, y = "is_popular")
```

From the exploratory data analysis functions that were run in this report I learned that 

* most articles were published during the week and not on the weekend (mainly Tue, Wed, Fri).
* the most popular channel is "world", followed by "tech" and "entertainment".
* the number of keywords is mostly between 5 and 10.
* most articles use zero or very few images and videos.

The report is available as a separate html document.

## Model Training

I am going to train five models, and compare their performance using AUC. The AUC statistic is the most commonly used measure for diagnostic accuracy of quantitative tests. It is a discrimination measure which tells us how well we can classify observations in two groups: those with and those without the outcome of interest. 

```{r}
# separate datasets
set.seed(482)
train_indices <- createDataPartition(train_data$is_popular, 
                                     p = 0.85, 
                                     list = FALSE)
temp_train <- train_data[train_indices, ]
temp_holdout <- train_data[-train_indices, ]

# initialize list
holdout_AUC <- list()

# variables
variables <- paste(" ~", paste(names(train_data[,-c(59,61)]), collapse = " + "))
```


### Linear Model

First I am training a Logit LASSO model as my benchmark model, a type of linear regression that uses shrinkage to reduce model complexity.

I am going to use different lambdas, which defines how much weight we want to add to penalization (the larger lambda the higher the penalty). Cross validation is used to check the best lambda.

The data is pre-processed before training, which includes centering and scaling the data.
I am going to set `metric` to "ROC" to choose models based on AUC.
                          
```{r lasso}
# tuning parameters
train_control <- trainControl(method = "cv", 
                         number = 5,
                         classProbs = TRUE,
                         summaryFunction = twoClassSummary)

lasso_tg <- expand.grid("alpha" = 1, 
                        "lambda" = 10^seq(-1, -4, length = 10))

set.seed(1102)
logit_lasso_model <- train(
    formula(paste0("is_popular_f", variables)),
    data = temp_train,
    method = "glmnet",
    preProcess = c("center", "scale"),
    family = "binomial",
    trControl = train_control,
    tuneGrid = lasso_tg,
    metric = "ROC"
  )

# print(logit_lasso_model)
```

```{r lasso_tuned}
tuned_logit_lasso_model <- logit_lasso_model$finalModel
best_lambda <- logit_lasso_model$bestTune$lambda
lasso_coeffs <- as.matrix(coef(tuned_logit_lasso_model, best_lambda))
```

The LASSO algorithm picked a regression with 49 predictor variables/interactions and `lambda` = 0.0004641589. 
I will use the trained model to predict the probability of an article being shared or not on the holdout dataset.
The default threshold for prediction is 50%. 
If we were to increase the threshold for predicting something to be positive, we will have fewer cases that we label as positive and therefor decrease the false positives and true positives. 


```{r}
# Probability prediction
holdout_prediction_probs <- predict.train(logit_lasso_model,
                                          newdata = temp_holdout, 
                                          type = "prob")
```

The ROC curve summarizes how a binary classifier performs "overall", taking into account all possible thresholds. It shows the trade-off between true positive rate (a.k.a sensitivity, # true positives / # all positives) and the false positive rate (a.k.a 1 - specificity, # false positive / # negatives).

```{r}
# ROC curve
rocr_prediction <- prediction(holdout_prediction_probs$yes,
                              temp_holdout[["is_popular_f"]])

plot(performance(rocr_prediction, "tpr", "fpr"), colorize=TRUE) 
```

AUC is the "area under the (ROC) curve". This is a number between 0 and 1. Higher AUC generally means better classification.

```{r}
# AUC
holdout_AUC[["logit_lasso"]] <- performance(rocr_prediction, 
                                            measure = "auc")@y.values[[1]]
# print(holdout_AUC[["logit_lasso"]])
```

The bigger the area under the ROC curve is, the better is our prediction. In this case we have an AUC of 0.7111.

```{r eval = FALSE}
# PREDICT on test_data
test_prediction_probs <- predict.train(logit_lasso_model, 
                                       newdata = test_data, 
                                       type = "prob")

# WRITE prediction on test set
filename <- paste0("predictions/",round(holdout_AUC[["logit_lasso"]]*1000),"_lasso.csv")
df <- data.frame(article_id = test_data$article_id,
                   score = test_prediction_probs[,2])
names(df) <- c("article_id", "score")
write.csv(df,file=filename,row.names=FALSE)
```

### Random Forest

Next, I am going to build a Probability Random Forest to predict y. 

Random Forest combines multiple decision trees, each fit to a random sample of the original data. It builds a predictive model without intervention to select variables or functional form. By combining multiple trees it reduces variance with minimal increase in bias.

I am going to use the default option of growing 500 trees, with several different tuning parameters, such as number of variables randomly chosen for any split in the tree or the minimum number of observations in the terminal nodes. To verify the optimal parameters I use 5-fold cross-validation. Here data scaling was not applied during training.

```{r random_forest, eval = FALSE}
# tuning parameters
train_control <- trainControl(method = "cv", 
                         number = 5,
                         classProbs = TRUE,
                         summaryFunction = twoClassSummary)

tune_grid <- expand.grid(
  .mtry = c(5, 7, 9),
  .splitrule = "gini",
  .min.node.size = c(10, 15)
)
# suggested mtry: square root of the number of predictor variables


# train model "ranger"
set.seed(654)
rf_model_p <- train(
  formula(paste0("is_popular_f", variables)),
  method = "ranger",
  data = temp_train,
  tuneGrid = tune_grid,
  trControl = train_control,
  metric = "ROC",
  verbose = FALSE
)

# rf_model_p
```

The performance of the random forest model changed only marginally with different tuning paramaters. The best results were produced with the following parameters: 5 features to consider for each split and minimum 10 observations in each terminal node.

```{r eval = FALSE}
# Probability prediction
holdout_prediction_probs <- predict.train(rf_model_p, 
                                          newdata = temp_holdout, 
                                          type = "prob")

# AUC
rocr_prediction <- prediction(holdout_prediction_probs$yes,
                              temp_holdout[["is_popular_f"]])
holdout_AUC[["random_forest"]] <- performance(rocr_prediction,
                                              measure = "auc")@y.values[[1]]
# print(holdout_AUC[["random_forest"]])
```
The random forest model produced an AUC of 0.7213, which is an improvement to the previous model (Logit Lasso AUC: 0.7111).

```{r eval = FALSE}
# PREDICT on test_data
test_prediction_probs <- predict.train(rf_model_p, 
                                       newdata = test_data, type = "prob")

# WRITE prediction on test set
filename <- paste0("predictions/",round(holdout_AUC[["random_forest"]]*1000),"_rf.csv")
df <- data.frame(article_id = test_data$article_id,
                 test_prediction_probs[,2])
names(df) <- c("article_id", "score")
write.csv(df,file=filename,row.names=FALSE)
```

### Gradient Boosting

Gradient boosting machines are also an ensemble of trees, however, the method of building the trees is different. The idea is to gradually (step-by-step) improve the trees by using the residuals of the previous tree. For not overfitting the data, a shrinkage parameter is constantly added, as well as a learning rate.

I am going to use different hyperparameters, which include the number of trees (the number of gradient boosting iteration), maximum nodes per tree, the learning rate and the minimum number of observations in trees' terminal nodes.

```{r gradient_boosting, eval = FALSE}
tune_grid <- expand.grid(n.trees = c(100, 500, 1000), 
                        interaction.depth = c(2, 3, 4, 6), 
                        shrinkage = c(0.005, 0.01, 0.1),
                        n.minobsinnode = c(5,10))

set.seed(1234)
gbm_model <- train(formula(paste0("is_popular_f", variables)),
                   method = "gbm",
                   data = temp_train,
                   trControl = train_control,
                   tuneGrid = tune_grid,
                   preProcess = c("center", "scale", "pca"),
                   metric = "ROC",
                   verbose = FALSE
                   )

# gbm_model
```

The final values used for the model were n.trees = 1000, interaction.depth = 6, shrinkage = 0.01 and n.minobsinnode = 10.

```{r eval = FALSE}
# Probability prediction
holdout_prediction_probs <- predict.train(gbm_model, 
                                          newdata = temp_holdout, 
                                          type = "prob")

# AUC
rocr_prediction <- prediction(holdout_prediction_probs$yes,
                              temp_holdout[["is_popular_f"]])

holdout_AUC[["gradient_boosting"]] <- performance(rocr_prediction, measure = "auc")@y.values[[1]]
# print(holdout_AUC[["gradient_boosting"]])
```

The gradient boosting model produced an AUC of 0.7067, which shows no improvement compared to the previous models (Logit Lasso AUC: 0.7111, Random Forest AUC: 0.7213).

I also tried to remove `pca` variable reduction, which improved the AUC to 0.7338. This might be an indicator that variables are not very much correlated, which I had already assumed from the correlation table in the DataExplorer report.

```{r eval = FALSE}
# PREDICT on test_data
test_prediction_probs <- predict.train(gbm_model, newdata = test_data, type = "prob")

# WRITE prediction on test set
filename <- paste0("predictions/",round(holdout_AUC[["gradient_boosting"]]*1000),"_gbm.csv")
df <- data.frame(article_id = test_data$article_id,
                   score = test_prediction_probs[,2])
names(df) <- c("article_id", "score")
write.csv(df,file=filename, row.names = FALSE)
```

### Neural Network

I am going to train my neural network model with the caret package and tune the hyper-parameters using cross validation. 
The hyperparameters of my neural net model are `size` and `decay`. Size is the number of units in hidden layer (nnet fit a single hidden layer neural network) and decay is the regularization parameter to avoid over-fitting. 
Data is preprocessed, meaning centered, scaled and dimensions are reduced using PCA.

```{r nnet, eval = FALSE}
tune_grid <- expand.grid(size = c(3, 5, 7, 10, 15),
                         decay = c(1, 1.5, 2, 2.5, 5, 7, 10))

set.seed(857)
nnet_model <- train(formula(paste0("is_popular_f", variables)),
                   method = "nnet",
                   data = temp_train,
                   trControl = train_control,
                   tuneGrid = tune_grid,
                   preProcess = c("center", "scale", "pca"),
                   metric = "ROC",
                   trace = FALSE) # avoid extensive iteration output

nnet_model
```

The final values used for the model were size = 7 and decay = 5.

```{r eval = FALSE}
# Probability prediction
holdout_prediction_probs <- predict.train(nnet_model,
                                          newdata = temp_holdout, 
                                          type = "prob")

# AUC
rocr_prediction <- prediction(holdout_prediction_probs$yes,
                              temp_holdout[["is_popular_f"]])

holdout_AUC[["nnet_model"]] <- performance(rocr_prediction, 
                                           measure = "auc")@y.values[[1]]
# print(holdout_AUC[["nnet_model"]])
```

The neural network model's AUC of 0.7137 is similar to the lasso model (Logit Lasso AUC: 0.7111, Random Forest AUC: 0.7213, Gradient Boosting AUC: 0.7067).

Again I tried to train the same model without using PCA as the variables, but it only marginally improved the result with an AUC of 0.7177.


```{r eval = FALSE}
# PREDICT on test_data
test_prediction_probs <- predict.train(nnet_model, newdata = test_data, type = "prob")

# WRITE prediction on test set
filename <- paste0("predictions/",round(holdout_AUC[["nnet_model"]]*1000),"_nnet.csv")
df <- data.frame(article_id = test_data$article_id,
                   score = test_prediction_probs[,2])
names(df) <- c("article_id", "score")
write.csv(df,file=filename,row.names=FALSE)
```

For comparison I am going to train a neural network model using H2O. H2O is an open source, in-memory, distributed, fast, and scalable machine learning and predictive analytics platform.

First I am going to connect to a local H2O instance.

```{r message=FALSE, warning = FALSE}
# Try to connect to a local H2O instance that is already running.
# If not found, start a local H2O instance from R with 8 gigabytes of memory.
h2o.init(max_mem_size = '8g') 
```

For this model I have chosen 3 hidden layers of 32 neurons each. This is a rather small network but it will run faster and hopefully still produces good results, as in the previous example also a small number of neurons was chosen by the algorithm. 
I have chosen 10000 epochs, but expect the misclassification rate to converge earlier, which will stop the training due to the early-stopping rule I will implement.

```{r nnet_h2o, eval = FALSE}
h2o_nnet_model <- h2o.deeplearning(y = "is_popular_f",
                             x = names(train_data[,-c(59,61)]),
                             training_frame = as.h2o(temp_train),
                             validation_frame = as.h2o(temp_holdout),
                             activation = "Rectifier",
                             hidden=c(32,32,32), 
                             epochs = 10000,
                             stopping_rounds=2,
                             stopping_metric="misclassification",
                             stopping_tolerance=0.01,
                             reproducible = TRUE,
                             nfolds = 5,
                             keep_cross_validation_predictions = TRUE,
                             seed = 123)

holdout_AUC[["h2o_nnet_model"]] <- h2o.auc(h2o_nnet_model, valid = TRUE)
# print(holdout_AUC[["h2o_nnet_model"]])
```

The h2o neural network model was not able to produce a better result than the neural net model trained with caret: 0.7031 (h2o) versus 0.7137 (caret).

```{r eval = FALSE}
test_prediction_probs <- h2o.predict(h2o_nnet_model, 
                                     newdata = as.h2o(test_data))

filename <- paste0("predictions/",round(holdout_AUC[["h2o_nnet_model"]]*1000),"_h2o_nnet.csv")
df <- data.frame("article_id" = test_data$article_id,
                 "score" = as.data.table(test_prediction_probs[,3]))
names(df) <- c("article_id", "score")
write.csv(df,file=filename,row.names=FALSE)
```

### Stacked model

Finally I am going to build a stacked model using h2o. 

I am going to combine four models of different families using cross validation for hyperparameter tuning. These are my **base models**. With these models I am going to approximate the target variable. With the output of each model I am going to use some kind of combining mechanism to combine the results: I am going to use the preditions of my base models as features to train a new model - my **metalearner**. With this combination I am going make my final prediction. The metalearner is supposed to highlight each base learner where it performs better and where it performs worse. In short: Stacking is a model ensembling technique used to combine information from multiple predictive models to produce a new one. 

*Model 1 - Elastic net:*  
In this model I am again using grid search with cross-validation to tune the hyperparamter `alpha` and then extract the best models in terms of cross validated AUC.

```{r stacked_elasticnet, eval = FALSE}
hyper_params <- list(alpha = c(0, .25, .5, .75, 1))

glm_grid <- h2o.grid(y = "is_popular_f",
                     x = names(train_data[,-c(59,61)]),
                     training_frame = as.h2o(temp_train),
                     validation_frame = as.h2o(temp_holdout),
                     algorithm = "glm",
                     lambda_search = TRUE,
                     nfolds = 5,
                     seed = 123,
                     family = "binomial",
                     hyper_params = hyper_params,
                     keep_cross_validation_predictions = TRUE,
                     grid_id = "1glm")

# Get the grid results, sorted by AUC
glm_gridperf <- h2o.getGrid("1glm",
                              sort_by = "auc",
                              decreasing = TRUE) 

# print(glm_gridperf)
```

```{r eval = FALSE}
# Grab the model_id for the top GLM model, chosen by best AUC
best_glm_model_id <- glm_gridperf@model_ids[[1]]
best_glm <- h2o.getModel(best_glm_model_id)

# get validation AUC
holdout_AUC[["h2o_glm"]] <- h2o.auc(best_glm, valid = TRUE)
# holdout_AUC[["h2o_glm"]]
```

The best model in terms of AUC was an elastic net model (`alpha` = 0.25) and produced an AUC of 0.7108 on the validation data set.

```{r eval = FALSE}
test_prediction_probs <- h2o.predict(best_glm, newdata = as.h2o(test_data))

filename <- paste0("predictions/",round(holdout_AUC[["h2o_glm"]]*1000),"_h2o_glm.csv")
df <- data.frame("article_id" = test_data$article_id,
                 "score" = as.data.table(test_prediction_probs[,3]))
names(df) <- c("article_id", "score")
write.csv(df,file=filename,row.names=FALSE)
```


*Model 2 - Random Forest:*
As in the previous random forest example I will be growing 500 trees. The default in h2o is 50 but I have incresed it because I will let the early stopping criteria decide when the random forest is sufficiently accurate. 
The algorithm will stop fitting new trees when the 2-tree average is within 0.001 (default) of the prior two 2-tree averages.
As part of hyperparameter tuning I am going to specify different number of columns to randomly select at each level.

```{r stacked_rf, eval=FALSE}
rf_params <- list(ntrees = c(500),
                  mtries = c(3, 5, 7, 9))

rf_grid <- h2o.grid(y = "is_popular_f",
                    x = names(train_data[,-c(59,61)]),
                    training_frame = as.h2o(temp_train),
                    validation_frame = as.h2o(temp_holdout), 
                    algorithm = "randomForest", 
                    nfolds = 5,
                    seed = 123,
                    keep_cross_validation_predictions = TRUE,
                    hyper_params = rf_params,
                    stopping_rounds = 2,
                    grid_id = "2rf")


# Get the grid results, sorted by AUC
rf_gridperf <- h2o.getGrid("2rf",
                              sort_by = "auc",
                              decreasing = TRUE)
# print(rf_gridperf)
```


```{r eval = FALSE}
# Grab the model_id for the top RF model, chosen by validation AUC
best_rf_model_id <- rf_gridperf@model_ids[[1]]
best_rf <- h2o.getModel(best_rf_model_id)

# get validation AUC
holdout_AUC[["h2o_rf"]] <- h2o.auc(best_rf, valid = TRUE)
# holdout_AUC[["h2o_rf"]]
```

The final model produced an AUC of 0.7278 on the validation set. It used five randomly selected columns at each level (`mtries` = 5) and 259 trees. 

```{r eval = FALSE}
test_prediction_probs <- h2o.predict(best_rf, newdata = as.h2o(test_data))

filename <- paste0("predictions/",round(holdout_AUC[["h2o_rf"]]*1000),"_h2o_rf.csv")
df <- data.frame("article_id" = test_data$article_id,
                 "score" = as.data.table(test_prediction_probs[,3]))
names(df) <- c("article_id", "score")
write.csv(df,file=filename,row.names=FALSE)
```

*Model 3 - Gradient Boosting:*

For this model I am going to adjust some of the default parameters and use grid search to find the optimal hyperparameters.
I will increase the number of trees from the default of 50 to 300. I will try different learning rates. Increasing the learning rate means the contribution of each tree will be stronger, so the model will move further away from the overall mean.
I will also try different values for depth which adjusts the "weakness" of each learner. Adding depth makes each tree fit the data closer. 

```{r stacked_gbm, eval=FALSE}
gbm_params <- list(learn_rate = c(0.01, 0.05, 0.1, 0.2),
                    max_depth = c(2, 3, 5, 10),
                    sample_rate = c(0.5), # row sample rate
                    col_sample_rate = c(0.5, 1.0))

# Train and validate a cartesian grid of GBMs
gbm_grid <- h2o.grid(y = "is_popular_f",
                     x = names(train_data[,-c(59,61)]),
                     training_frame = as.h2o(temp_train),
                     validation_frame = as.h2o(temp_holdout), 
                     algorithm = "gbm", 
                     nfolds = 5,
                     seed = 123,
                     ntrees = 300,
                     keep_cross_validation_predictions = TRUE,
                     hyper_params = gbm_params,
                     stopping_rounds = 2,
                     grid_id = "3gbm")

# Get the grid results, sorted by AUC
gbm_gridperf <- h2o.getGrid("3gbm",
                              sort_by = "auc",
                              decreasing = TRUE)
# print(gbm_gridperf)
```


```{r eval = FALSE}
# Grab the model_id for the top GBM model, chosen by validation AUC
best_gbm_model_id <- gbm_gridperf@model_ids[[1]]
best_gbm <- h2o.getModel(best_gbm_model_id)

holdout_AUC[["h2o_gbm"]] <- h2o.auc(best_gbm, valid = TRUE)
# holdout_AUC[["h2o_gbm"]]
```

The best GBM model produced an AUC of 0.7307 on the validation set. It used a learning rate of 0.01, max_depth = 10, col_sample_rate of 0.5 and 295 number of trees.

```{r eval = FALSE}
test_prediction_probs <- h2o.predict(best_gbm, newdata = as.h2o(test_data))

filename <- paste0("predictions/",round(holdout_AUC[["h2o_gbm"]]*1000),"_h2o_gbm.csv")
df <- data.frame("article_id" = test_data$article_id,
                 "score" = as.data.table(test_prediction_probs[,3]))
names(df) <- c("article_id", "score")
write.csv(df,file=filename,row.names=FALSE)
```

*Model 4 - Deep learning:*  
I am going to include the previously trained neural net in my stacked model.

*Validation performances:*  
Here is a summary of performances (evaluated on the validation set) for each of the different models  and its optimal hyperparameters.

```{r eval = FALSE, include = FALSE}
# predict on validation set
validation_performances <- list(
  "glm" = glm_AUC,
  "rf" = rf_AUC,
  "gbm" = gbm_AUC,
  "dl" = holdout_AUC[["nnetH2O_model"]]
)

kable(data.frame(validation_performances))

# kable(data.frame(holdout_AUC)) %>% 
#   kable_styling()  %>% 
#   column_spec(5:8,bold = T, color = "white", background = "darkgrey")
```

```{r eval=FALSE}
kable(data.frame(holdout_AUC)) %>%
  kable_styling()
```
```{r echo=FALSE}
kable(data.frame(h2o_glm = 0.7108,
                 h2o_rf = 0.7280,
                 h2o_gbm = 0.7307,
                 h2o_nnet = 0.7031)) %>%
  kable_styling()
```

```{r include = FALSE, eval = FALSE}
# *Correlations of predicted scores*
# Now I am going to check how large the correlations of predicted scores produced by the base learners are.
ensemble_model <- h2o.stackedEnsemble(
                     y = "is_popular_f",
                     x = names(train_data[,-c(59,61)]),
                     training_frame = as.h2o(temp_train),
                     validation_frame = as.h2o(temp_holdout), 
                     base_models = list(best_glm,
                                        best_rf,
                                        best_gbm,
                                        h2o_nnet_model),
                     keep_levelone_frame = TRUE)

level_1_features <- h2o.getFrame(ensemble_model@model$levelone_frame_id$name)

# inspect correlations among scores from different models
level_1_dt <- as.data.table(level_1_features)[, 1:4]
setnames(level_1_dt, c("glm", "rf", "gbm", "nnet"))

ggcorr(level_1_dt, label = TRUE, label_round = 2)

#Both ensemble decision tree models (random forest and gradient boosting machines) are highly correlated, the others less so.
```

*Stacked ensemble model:*  
Recall, stacking is a combination of predictive models. The idea is that combined models should produce something more stable and have larger predictive performance (decrease the variance).

To produce a stacked ensemble model I am going to define several independent models and use the same folds for training each model. Then I do an out of fold prediction (produce prediction for these left out folds) and record the value we predicted as new features. With another predictive model I use these new features to estimate the outcome based on these baseline predictive scores. I am stacking the out of fold predictions and produce one single score from those features.

First I start with the baseline meta-learner - a glm model with non-negative weights (coefficients are constraints to be non-negative).

```{r stacked_metaglm, eval = FALSE}
ensemble_model_glm <- h2o.stackedEnsemble(
                     y = "is_popular_f",
                     x = names(train_data[,-c(59,61)]),
                     training_frame = as.h2o(temp_train),
                     validation_frame = as.h2o(temp_holdout), 
                     base_models = list(best_glm,
                                        best_rf,
                                        best_gbm,
                                        h2o_nnet_model))
```

Then I use "gbm" as meta-learner.

```{r stacked_metagbm, eval = FALSE}
ensemble_model_gbm <- h2o.stackedEnsemble(
                     y = "is_popular_f",
                     x = names(train_data[,-c(59,61)]),
                     training_frame = as.h2o(temp_train),
                     validation_frame = as.h2o(temp_holdout), 
                     base_models = list(best_glm,
                                        best_rf,
                                        best_gbm,
                                        h2o_nnet_model),
                     metalearner_algorithm = "gbm")
```

*Validation Performances:*

```{r eval = FALSE}
validation_performances <- list()
validation_performances[["ensemble_glm"]] = h2o.auc(h2o.performance(ensemble_model_glm, valid = TRUE))
validation_performances[["ensemble_gbm"]] = h2o.auc(h2o.performance(ensemble_model_gbm, valid = TRUE))

kable(data.frame(validation_performances)) %>% 
  kable_styling(full_width = FALSE)
  # column_spec(1:2,bold = T, color = "white", background = "darkgrey")
```

```{r echo=FALSE}
validation_performances <- list()
validation_performances[["ensemble_glm"]] = 0.735
validation_performances[["ensemble_gbm"]] = 0.713

kable(data.frame(validation_performances)) %>% 
  kable_styling(full_width = FALSE)
  # column_spec(1:2,bold = T, color = "white", background = "darkgrey")
```
The first ensemble, using glm as meta-learner performed better than elastic net, random forest, gbm and deep learning.

```{r eval = FALSE}
# PREDICT on test_data
test_prediction_probs <- h2o.predict(ensemble_model_glm, newdata = as.h2o(test_data))

# WRITE prediction on test set
filename <- paste0("predictions/",round(validation_performances[["ensemble_glm"]]*1000),"_stacking.csv")
df <- data.frame("article_id" = test_data$article_id,
                 "score" = as.data.table(test_prediction_probs[,3]))
names(df) <- c("article_id", "score")
write.csv(df,file=filename,row.names=FALSE)
```

For my final model I therefor choose a stacked ensemble model with glm as meta learner and train it on the entire training dataset to predict my final results:

## FINAL MODEL

```{r stacked_final, eval = FALSE}
# elastic net
# The best model in terms of AUC was an elastic net model (alpha = 0.25)
h2o_glm_x <- h2o.glm(y = "is_popular_f",
                     x = names(train_data[,-c(59,61)]),
                     training_frame = as.h2o(train_data),
                     lambda = 0.0004935,
                     family = "binomial",
                     alpha = c(.25),
                   seed = 700)

# random forest
# The final model used five randomly selected columns at each level (mtries = 5) and 259 trees. 
h2o_rf_x <- h2o.randomForest(y = "is_popular_f",
                    x = names(train_data[,-c(59,61)]),
                    training_frame = as.h2o(train_data),
                    stopping_rounds = 2,
                    ntrees = 300,
                    mtries = 5,
                   seed = 700)

# GBM
# The best GBM model used a learning rate of 0.01, max_depth = 10, col_sample_rate of 0.5 and 295 number of trees.
h2o_gbm_x <- h2o.gbm(y = "is_popular_f",
                   x = names(train_data[,-c(59,61)]),
                   training_frame = as.h2o(train_data),
                   ntrees = 300,
                   learn_rate = 0.01,
                   max_depth = 10,
                   sample_rate = 0.5,
                   col_sample_rate = 0.5,
                   stopping_rounds = 2,
                   seed = 700)

# neural net
h2o_nnet_x <- h2o.deeplearning(y = "is_popular_f",
                             x = names(train_data[,-c(59,61)]),
                             training_frame = as.h2o(train_data),
                             activation = "Rectifier",
                             hidden=c(32,32,32), 
                             epochs = 10000,
                             stopping_rounds=2,
                             stopping_metric="misclassification", ## could be "MSE","logloss","r2"
                             stopping_tolerance=0.01,
                             reproducible = TRUE,
                             seed = 700)

ensemble_model_glm_x <- h2o.stackedEnsemble(
                              y = "is_popular_f",
                              x = names(train_data[,-c(59,61)]),
                              blending_frame = as.h2o(train_data),
                              base_models = list(h2o_nnet_x, 
                                                 h2o_gbm_x, 
                                                 h2o_rf_x, 
                                                 h2o_glm_x))

test_prediction_probs <- h2o.predict(ensemble_model_glm_x, newdata = as.h2o(test_data))

filename <- paste0("predictions/final_stacking_x.csv")
df <- data.frame("article_id" = test_data$article_id,
                 "score" = as.data.table(test_prediction_probs[,3]))
names(df) <- c("article_id", "score")
write.csv(df,file=filename,row.names=FALSE)
```

```{r include = FALSE}
# h2o.shutdown()
```

## Summary

The dataset provided was clean and complete, so no further data cleaning steps were necessary prior to model building. Data was often preprocessed, meaning centered, scaled and dimensions were reduced using PCA, since correlated variables are problematic for gradient-based optimization. PCA however worsened model performance. 

Using the caret package, I trained a LASSO model, a random forest model, a gradient boosting machine model and a neural net. Among these random forest had the best results in terms of AUC on the validation set. 
Using h2o I trained an elastic net model, a random forest, a gradient boosting machine model and a neural net and stacked them using both glm and gbm as a meta learner. The stacked model using glm as a metalearner produced the best results in terms of AUC on the validation set. Among the base models gbm showed the best results, while gbm using caret was rather weak in predicting the target variable.

In the Kaggle competition my gbm model trained with h2o scored the best: the public score was 0.70587.


## Acknowledgments
The dataset used in the competition can be found at the UCI repository. We thank Kelwin Fernandes, Pedro Vinagre, Paulo Cortez and Pedro Sernadela for making it publicly available. Check also their publication below.

K. Fernandes, P. Vinagre and P. Cortez. A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News. Proceedings of the 17th EPIA 2015 - Portuguese Conference on Artificial Intelligence, September, Coimbra, Portugal.



```{r notes, include = FALSE}
# from keras import regularizers
# model.add(Dense(64, input_dim=64,
#                 kernel_regularizer=regularizers.l2(0.01),
#                 activity_regularizer=regularizers.l1(0.01)))

# Evolutionary Computing book: https://www.springer.com/gp/book/9783642072857

# https://www.researchgate.net/publication/216300585_Introduction_To_Evolutionary_Computing
```



```